from mab import MabAdversarial
import matplotlib.pyplot as plt
import math
import streamlit as st
import numpy as np
from PIL import Image
import time
import pandas as pd
import plotly.graph_objects as go

st.set_page_config("MAB-Adversarial", "üêô", layout="wide")
image = Image.open('img/mab.png')
logo = Image.open('img/logo-unifil.png')
col1, col2, col3 = st.columns([6, 18, 6])



texto_apresentacao = "  O c√≥digo implementa o problema do Multi-Armed Bandit Adversarial em um ambiente simulado. O usu√°rio pode definir o n√∫mero de bra√ßos e o n√∫mero de intera√ß√µes para o problema. O objetivo do agente √© escolher o bra√ßo que maximiza a recompensa ao longo do tempo. Cada bra√ßo tem uma recompensa associada desconhecida. O algoritmo utilizado √© o exp3, que leva em conta tanto a recompensa obtida quanto a explora√ß√£o de bra√ßos menos conhecidos. A cada intera√ß√£o, o algoritmo atualiza as estimativas das recompensas e os pesos dos bra√ßos. O c√≥digo exibe gr√°ficos da evolu√ß√£o das recompensas m√©dias, do n√∫mero de vezes que cada bra√ßo foi usado e do arrependimento acumulado ao longo do tempo."
texto_explicativo = "O c√≥digo implementa o problema do Multi-Armed Bandit Adversarial em um ambiente simulado. O usu√°rio pode definir o n√∫mero de bra√ßos e o n√∫mero de intera√ß√µes para o problema. O objetivo do agente √© escolher o bra√ßo que maximiza a recompensa ao longo do tempo. Cada bra√ßo tem uma recompensa associada desconhecida. O algoritmo utilizado √© o exp3, que leva em conta tanto a recompensa obtida quanto a explora√ß√£o de bra√ßos menos conhecidos. A cada intera√ß√£o, o algoritmo atualiza as estimativas das recompensas e os pesos dos bra√ßos. O c√≥digo exibe gr√°ficos da evolu√ß√£o das recompensas m√©dias, do n√∫mero de vezes que cada bra√ßo foi usado e do arrependimento acumulado ao longo do tempo."
with col2:
   st.image(image, width=None)
with col3:
   st.image(logo)
with col2:  
   st.markdown("<h1 style='text-align: center;'>Introdu√ß√£o</h1>", unsafe_allow_html=True)
   st.write(texto_explicativo)
   st.markdown("<h1 style='text-align: center;'>Defina as vari√°veis:</h1>", unsafe_allow_html=True)
   
  
co1, co2, co3, co4, co5 = st.columns([6, 8, 1, 8, 6])  
   
with co2:
    numArms = st.number_input("N√∫mero de Bra√ßos", value=10, min_value=1, max_value=10)
    button1 = st.button('Saber mais üîé', key='button1') 
    if button1:
        st.write("Representa o n√∫mero de alavancas dispon√≠veis para o agente puxar. Em outras palavras, √© o n√∫mero de op√ß√µes que o agente tem para escolher a cada vez que ele precisa tomar uma decis√£o.")
    maxReward = st.number_input("M√°ximo de Recompensa", value=1, min_value=1, max_value=10)
    button2 = st.button('Saber mais üîé', key='button2') 
    if button2:
        st.write("Representa o valor m√°ximo de recompensa que o agente pode receber ao puxar uma determinada alavanca. Em outras palavras, √© o valor m√°ximo que o agente pode receber por escolher a melhor op√ß√£o.")
    T = st.number_input("N√∫mero de Intera√ß√µes", value=100, min_value=1, max_value=10000)
    button3 = st.button('Saber mais üîé', key='button3')
    if button3:
        st.write("Representa o n√∫mero de vezes que o agente pode puxar as alavancas. Quanto maior o n√∫mero de intera√ß√µes, maior a oportunidade para o agente aprender e aprimorar sua estrat√©gia de escolha.")
with co4:    
    gamma = st.slider("Taxa de Explora√ß√£o/Explota√ß√£o", min_value=0.0, max_value=1.0, value=0.5, step=0.01)
    button4 = st.button('Saber mais üîé', key='button4')
    if button4:
        st.write("Representa a propor√ß√£o entre a escolha de uma alavanca com base no valor que ela oferece (explora√ß√£o) e a escolha de uma alavanca com base em sua frequ√™ncia de escolha at√© o momento (explota√ß√£o). Uma taxa de explora√ß√£o alta significa que o agente est√° disposto a experimentar op√ß√µes diferentes, enquanto uma taxa de explora√ß√£o baixa significa que ele est√° se concentrando nas op√ß√µes que lhe deram as melhores recompensas at√© o momento.")    
    adversarial_prob=A = st.slider("Chance de Haver Advers√°rios", min_value=0.0, max_value=1.0, value=0.10, step=0.01)
    button5 = st.button('Saber mais üîé', key='button5')
    if button5:
        st.write("Representa a possibilidade de que o pr√≥prio ambiente do MAB possa ser enganoso ou inst√°vel, fornecendo feedbacks incorretos ou vari√°veis")


co1, co2, co3, co4, co5 = st.columns([6, 8, 3, 8, 6]) 
coln1, coln2, coln3 = st.columns([1, 3, 1,])

with co3:
    if st.button('Executar'):
        with coln2:
            with st.spinner('Realizando os calculos...'):
                time.sleep(2)
                mab = MabAdversarial(numArms=numArms, maxReward=maxReward, gamma=gamma, T=T, adversarial_prob=adversarial_prob)
                mean_rewards, arm_counts, allRewards, regret, fakeRewards, results = mab.run()
                mean_rewards2, arm_counts2, allRewards2, regret2, results2 = mab.run_no_adversarial()
            st.success('Conclu√≠do!')
            
            rewards1, rewards2 = zip(*fakeRewards)
            data = {'Bra√ßo': [f'Bra√ßo {i+1}' for i in range(len(arm_counts))],
                    'Sele√ß√µes': arm_counts,
                    'Recompensas M√©dias': mean_rewards,
                    'Recompensas Min Estimadas': rewards1,
                    'Recompensas Max Esperada': rewards2}
            df = pd.DataFrame(data)
            st.write(df)

            data = {'Bra√ßo': [f'Bra√ßo {i+1}' for i in range(len(arm_counts2))],
                    'Sele√ß√µes': arm_counts2,
                    'Recompensas M√©dias': mean_rewards2,
                    'Recompensas Min Estimadas': rewards1,
                    'Recompensas Max Esperada': rewards2}
            df2 = pd.DataFrame(data)
            st.write(df2)


            total_selecoes = sum(arm_counts)
            porcentagens = [count / total_selecoes * 100 for count in arm_counts]
            labels = [f'Bra√ßo {i+1}' for i in range(len(porcentagens))]
            fig = go.Figure(data=[go.Pie(labels=labels, values=porcentagens, hole=0.3)])
            fig.update_layout(title='Porcentagem de sele√ß√µes por bra√ßo')
            st.plotly_chart(fig)

            total_selecoes = sum(arm_counts2)
            porcentagens = [count / total_selecoes * 100 for count in arm_counts2]
            labels = [f'Bra√ßo {i+1}' for i in range(len(porcentagens))]
            fig = go.Figure(data=[go.Pie(labels=labels, values=porcentagens, hole=0.3)])
            fig.update_layout(title='Porcentagem de sele√ß√µes por bra√ßo sem Adversarial')
            st.plotly_chart(fig)

            df = pd.DataFrame(results, columns=['Intera√ß√£o', 'Bra√ßo escolhido', 'Recompensa'])
            recompensas_bra√ßos = {}
            for bra√ßo in range(numArms):
                recompensas = df.loc[df['Bra√ßo escolhido'] == bra√ßo, 'Recompensa']
                recompensas_bra√ßos[bra√ßo] = recompensas

            fig = go.Figure()
            for bra√ßo, recompensas in recompensas_bra√ßos.items():
                fig.add_trace(go.Scatter(x=recompensas.index, y=recompensas, mode='lines', name=f'Bra√ßo {bra√ßo+1}'))
            fig.update_layout(title='Grafico de Recompensas', xaxis_title='Intera√ß√£o', yaxis_title='Recompensa')
            st.plotly_chart(fig)

            df = pd.DataFrame(results2, columns=['Intera√ß√£o', 'Bra√ßo escolhido', 'Recompensa'])
            recompensas_bra√ßos = {}
            for bra√ßo in range(numArms):
                recompensas = df.loc[df['Bra√ßo escolhido'] == bra√ßo, 'Recompensa']
                recompensas_bra√ßos[bra√ßo] = recompensas

            fig = go.Figure()
            for bra√ßo, recompensas in recompensas_bra√ßos.items():
                fig.add_trace(go.Scatter(x=recompensas.index, y=recompensas, mode='lines', name=f'Bra√ßo {bra√ßo+1}'))
            fig.update_layout(title='Grafico de Recompensas sem o Adversarial', xaxis_title='Intera√ß√£o', yaxis_title='Recompensa')
            st.plotly_chart(fig)





